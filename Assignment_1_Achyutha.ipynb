{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Assignment-1 .ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ844o3Oo_y9"
      },
      "source": [
        "# CS 5683 - Big Data Analytics\n",
        "## Assignment - 1: Intro. to Spark and RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3Xvw-rio_y_"
      },
      "source": [
        "###### Use Google Colab to use this notebook\n",
        "###### Let's setup Spark first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUIRsN79o_zA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea83f1b-b640-4eb3-8eb2-2c2e1defdd5f"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 65 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 51.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=a6953a912734b4ab5580657fcaf09f4e148f40ae807a8fea5fd42d31c0ddd769\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXjnu07mo_zC"
      },
      "source": [
        "###### Import required libraries now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzZN0SjIo_zD"
      },
      "source": [
        "import sys\n",
        " \n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcK6bASio_zD"
      },
      "source": [
        "###### Let's initialize Spark context now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTGUvTySo_zE"
      },
      "source": [
        "# create Spark context with necessary configuration\n",
        "sc = SparkContext(\"local\",\"PySpark - CS5683 - Assignment-1\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkvlhCDCo_zE"
      },
      "source": [
        "###### Follow the tutorial to mount your Google Drive. Give mounted Drive paths below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNN62IhVo_zF"
      },
      "source": [
        "# Give **.txt FILE PATHS** here\n",
        "# Use your own files which could match your application\n",
        "file1 = 'file1.txt'\n",
        "file2 = 'file2.txt'\n",
        "\n",
        "# USE THESE FILES as input(s) FOR ALL BELOW QUESTIONS"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuzTmS-no_zG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cQDb9z9o_zH"
      },
      "source": [
        "### Example Spark program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ysC-Ujxo_zH"
      },
      "source": [
        "# Example Spark application for a simple wordcount\n",
        "# What is wordcount? \n",
        "    # Given a file, count the frequency of all words appearing in that file\n",
        "    \n",
        "# Step-1: Read the required file. In our case it is file1 or file2.\n",
        "# NOTE: We do not need to initialize SparkContext as only one SparkContext can be initialized in one notebook\n",
        "fileRDD = sc.textFile(file1)\n",
        "\n",
        "# Step-2: \n",
        "    # Each line in our file(s) is a sentence. So, we need to split the sentence with ' ' to get words\n",
        "    # Using map() will return RDD[list]. But we need RDD[string]. So we use flatMap()\n",
        "wordsRDD = fileRDD.flatMap(lambda line: line.split(\" \")) # <----------- TEST what happens when you use map()\n",
        "\n",
        "# Step-3: For each input, we will make (K,V) pair, where K is the word and V is 1\n",
        "pairRDD = wordsRDD.map(lambda word: (word,1))\n",
        "\n",
        "# Step-4: Now we have to sum all 1's of each word\n",
        "# NOTE: A word may present in multiple data partitions. So we use reduceByKey() to group by key and perform sum\n",
        "countRDD = pairRDD.reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "#Step-5: Save results in a text file\n",
        "countRDD.saveAsTextFile('') # <----------- GIVE FILE PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6LHqn7Ro_zI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92KMLdugo_zJ"
      },
      "source": [
        "### Question - 1 (10 points) "
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "PGzcYDMxo_zJ"
      },
      "source": [
        "Write a Spark application that counts number of non-unique words that begins with each special character in file1. That is count the number of words (including duplicates) that begins with each special characters. You can **ignore** letter cases (consider the given text contains only lower-case letters). \n",
        "\n",
        "Example Output:\n",
        "('#', 500)\n",
        "('[', 100)\n",
        "\n",
        "which means that the given input file has 500 words that begin with the special character '#' and 100 words that begin with the special character '['.\n",
        "NOTE: The application counts duplicate words also and we need to use Python's regex to find special characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXgx5Qvdo_zK"
      },
      "source": [
        "# YOUR CODE for Question-1 HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-_S4CnFltGF",
        "outputId": "95d737fc-8fdb-436c-cee9-67716000cdf7"
      },
      "source": [
        "import re\n",
        "file1_rdd = sc.textFile(file1)\n",
        "file1_flat = file1_rdd.flatMap(lambda x: x.split(\" \"))\n",
        "\n",
        "\n",
        "file1_initial = file1_flat.filter(lambda x: x.split()!=\" \") \n",
        "                      \n",
        "file1_step1 = file1_initial.filter(lambda x: len(x) != 1)\n",
        "\n",
        "regex_string = '\\A[>?/\\|}\\]-_`$%{~:;\\[^&*()<@_!#]'\n",
        "file1_filter = file1_step1.filter(lambda x: re.search(regex_string, x))\n",
        "# Counting the character count\n",
        "file1_pre = file1_filter.filter(lambda x: x.strip()!='').map(lambda x: x[0])\n",
        "file1_map = file1_pre.map(lambda x: (x, 1))\n",
        "file1_count = file1_map.reduceByKey(lambda x,y: x+y)\n",
        "file1_count.collect()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[', 1),\n",
              " ('{', 1),\n",
              " ('}', 2),\n",
              " ('#', 1),\n",
              " ('*', 3),\n",
              " ('&', 1),\n",
              " ('^', 1),\n",
              " ('%', 1),\n",
              " ('$', 1),\n",
              " (';', 2),\n",
              " ('!', 2),\n",
              " ('@', 3),\n",
              " ('(', 1),\n",
              " (')', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA8W8wlWm5cj"
      },
      "source": [
        "# PRINT THE OUTPUT HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw-shNeVo_zL"
      },
      "source": [
        "### Question - 2 (10 points)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "G2Mg9Xgao_zL"
      },
      "source": [
        "Write a Spark application that counts the number of unique words in each line  in file1. You **do not** need to remove any non-alphabetic letters and special characters for this application. Similarly, you can **ignore** letter cases (consider the given text contains only lower-case letters)\n",
        "\n",
        "Example Output:\n",
        "(1, 100)\n",
        "(2, 700)\n",
        "(3, 1500)\n",
        "\n",
        "which means that the input file for the Spark application has 100 unique words in line 1, 700 unique words in line 2 , and 1500 uniqe words in line 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm6jNXLro_zL"
      },
      "source": [
        "# YOUR CODE for Question-2 HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtkeLZnLo_zL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "815d23a3-b24b-4b7d-84de-274b02ab2852"
      },
      "source": [
        "file1_index = file1_rdd.zipWithIndex() \\\n",
        "                       .map(lambda x: (int(x[1])+1, x[0]))\n",
        "\n",
        "file1_flat = file1_index.map(lambda x: (x[0], len(set(x[1].split(\" \"))))) \\\n",
        "                        .sortByKey()    \n",
        "                    \n",
        "file1_flat.collect()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 8), (2, 7), (3, 8), (4, 5), (5, 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-4-Irjro_zL"
      },
      "source": [
        "# PRINT THE OUTPUT HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCWDg-_Oo_zL"
      },
      "source": [
        "### Question - 3 (10 points)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "fBSdoL84o_zL"
      },
      "source": [
        "Write a Spark aplication that outputs wordcount from two files: file1 and file2. That is count the number of occurances of words from both file1 and file2. You can **ignore** letter cases (consider the given input files contain only lower-case letters). Also, you can **ignore** words that is not present in both files. **Sort the output in alphabetical order**\n",
        "\n",
        "Example Output:\n",
        "(big, (10, 20))\n",
        "(Data, (30, 50))\n",
        "\n",
        "which means the word \"big\" appears 10 times in file-1 and 20 times in file-2 and the word \"Data\" appears 30 times in file-1 and 50 times in file-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXGPyOq6o_zM"
      },
      "source": [
        "# YOUR CODE for Question-3 HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJH2N24SrcOo",
        "outputId": "5b038c5c-0df2-4c11-da62-3f9baa5b17af"
      },
      "source": [
        "file1_rdd10 = sc.textFile(file1)\n",
        "file2_rdd11 = sc.textFile(file2)\n",
        "file1_flat1 = file1_rdd10.flatMap(lambda x: x.split(\" \"))\n",
        "\n",
        "file2_flatmap1= file2_rdd11.map(lambda x: x.split(\" \")) \\\n",
        "                         .flatMap(lambda x: x)\n",
        "\n",
        "\n",
        "file1_count = file1_flat1.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y) \\\n",
        "                         .filter(lambda x: x[0]!='')\n",
        "\n",
        "file2_count = file2_flatmap1.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y) \\\n",
        "                         .filter(lambda x: x[0]!='')\n",
        "\n",
        "# Joining the data\n",
        "rdd_final = file1_count.join(file2_count) \\\n",
        "                       .sortBy(lambda x: x[0])\n",
        "\n",
        "rdd_final.collect()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('!jerry', (2, 1)),\n",
              " ('$jump', (1, 1)),\n",
              " ('%yup', (1, 1)),\n",
              " ('(as', (1, 1)),\n",
              " ('*1', (1, 1)),\n",
              " ('*up', (2, 2)),\n",
              " ('4567', (1, 1)),\n",
              " ('8kill', (2, 2)),\n",
              " (';to', (1, 1)),\n",
              " ('@jerry', (2, 1)),\n",
              " ('[ab', (1, 1)),\n",
              " ('^kill', (1, 1)),\n",
              " ('as', (1, 1)),\n",
              " ('fight', (1, 1)),\n",
              " ('jump', (1, 1)),\n",
              " ('kill', (1, 1)),\n",
              " ('opd', (1, 1)),\n",
              " ('pop', (1, 1)),\n",
              " ('tom', (1, 1)),\n",
              " ('yup', (1, 1)),\n",
              " ('}lol', (2, 1))]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHdSsUcFpfKQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s3wLBRro_zM"
      },
      "source": [
        "# PRINT THE OUTPUT HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwwyZ_Ako_zM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwG0Dvrxo_zN"
      },
      "source": [
        "### WHAT TO TURN-IN IN CANVAS"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "i8H4zB4Lo_zN"
      },
      "source": [
        "1. Complete questions 1,2, and 3\n",
        "2. Download this completed python notebook as .ipynb\n",
        "3. Upload it in Canvas assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOQjpTN-o_zO"
      },
      "source": [
        "# Due Date: Sept. 1 at 11:59pm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRt7O_bYo_zO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}